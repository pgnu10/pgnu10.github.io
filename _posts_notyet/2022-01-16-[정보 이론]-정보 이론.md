---
title: '[정보 이론] 정보 이론'
excerpt: 정보 이론에 대해 알아봅니다.
categories:
    - 정보 이론
tags:
    - 정보 이론
    - 비전공자를 위한
toc: true
toc_sticky: true
date: '2022-01-15'
lastmod: '2022-01-16'
---

<p align="center"><img src="/assets/images/question-mark-gdcf4fc51c_640.jpg"></p>
<span style="color:gray"><a href="https://pixabay.com/ko/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2492009">Pixabay</a>로부터 입수된 <a href="https://pixabay.com/ko/users/qimono-1962238/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2492009">Arek Socha</a>님의 이미지 입니다.</span>

# 정보 이론 Information Theory

## 개요

1948년, 미국의 전기공학자이자 수학자인 Claude Shannon은 현대 정보 이론의 토대가 된 논문 "**A Mathematical Theory of Communication (1948)**"을 발표했습니다. 이 논문은 정보를 숫자로 표현하는 방법에 대해서 고찰하여, 정보량 (information content)이라는 개념을 통해 통신(communication)의 효율화를 비롯한 정보 전달의 여러 문제들을 해결할 수 있음을 시사하였습니다. 이후 섀넌은 이 논문을 통해 정보 이론의 아버지로 불리게 됩니다.

정보 이론은 데이터의 양(=정보)을 숫자로 표현하여 최대한 많은 양의 데이터를 효율적으로 전송, 혹은 저장하려면 어떻게 해야할 지를 고민하기 위한 응용 수학의 한 분야입니다. 이를 위해 정보 이론에서는 데이터를 '정보 엔트로피 Entropy'라는 단위로 표현하여 계산합니다. 이 (섀넌의) 엔트로피는 볼츠만의 엔트로피와 같은 의미를 갖고 있습니다.

정보 이론은 정보 전송 용량 계산, 데이터의 부호화, 확률 분포 간의 유사성 측정 등에 폭넓은 용도로 활용됩니다.

이 글에서는 이해를 돕기 위해 Entropy 대신 "정보량"이라는 단어를 사용하겠습니다.

## 자주 발생하는 사건 < 자주 발생하지 않는 사건

정보 이론에서는 '자주 발생하는 사건'보다 '자주 발생하지 않는 사건'의 정보량이 더 많다(=informative)고 판단합니다. 예를 들어 '아이가 제시간에 귀가했다'보다 '아이가 제시간에 귀가하지 않았다' 쪽을 더 정보량이 많다고 판단하는 것입니다. 여기서 '자주 발생하는 사건'이란 그 발생 확률이 높은 사건, 즉 과거에 많이 발생했었던 사건을 의미합니다.

정보란 수집된 데이터를 실제 문제를 해결하기 위해 정리한 지식을 말합니다. 즉, 문제 해결을 위한 인간 본위적인 데이터입니다. 그래서 이미 과거에 그 해결이 이루어진 '자주 발생했던 사건'보다는 해결법이 수립되지 않았을 '자주 발생하지 않았던 사건'에의 중요도가 높은 것입니다.

## 2진법 (비트 Bit)
<p align="center" style="color:gray"><img src="/assets/images/yesno.webp">7nuit / Getty Images</p>


섀넌은 사건의 발생을 나타내기 위한 단위로 2진법 (비트 Bit) 를 사용했습니다. **비트**는 0과 1로 이루어진 정보의 단위입니다. 우리 인간이 사용하는 언어로 풀어 설명하자면 "예"와 "아니요"로 이루어진 단위입니다. 양자역학 혹은 패러독스와 같은 경우를 제외하면, 우리 주변의 모든 질문에 대한 대답은 "예"와 "아니요"로 정해집니다. '밥 먹었니?', '밖에 비가 오나요?', '수학 성적이 80점보다 높니?' 등 다양한 질문에 대해 대답하기 위해서는 "예", "아니요" 둘 중 하나만으로도 충분합니다. 즉, **융통성**이 높습니다.

비트는 정보 전달에서 확실함을 갖게 합니다. "예"와 "아니요"는 의미상으로 결코 양립할 수 없습니다. 만약 대답이 "예"라면 그 의미 안에는 결코 "아니요"가 포함될 수 없고, 이는 반대도 마찬가지 입니다. 현대 사회에서 정보 전달은 주로 전기를 통해 이루어지는 점을 생각해보면 이는 정말 큰 장점임을 알 수 있습니다. "전기가 흐르거나", "흐르지 않거나" 라는 두 상태로 비트를 표현할 수 있기 때문입니다. "전기가 중간 세기로 흐른다"와 같은 애매모호함을 배제하여 **명확한 정보 전달**을 가능하게 합니다.

또한 비트는 '0이 아니면 1'이라는 양자 택일의 성질 덕분에 사건의 발생 확률 p 하나로 그 값을 표현할 수 있습니다. 어떤 사건이 발생할 확률이 p 라면, 그 사건이 발생하지 않을 확률은 1-p 가 됩니다. 이로써 특정 사건에 대한 발생을 1 Bit 라는 매우 적은 정보량에 **효율적**으로 담을 수 있게 됩니다.

n 비트는 $$2^n$$ 가지의 정보를 표현할 수 있습니다. 예를 들어, 2 Bit를 이용하면 [00, 01, 10, 11]의 4 가지 정보를 표현할 수 있습니다. 반대로 생각해보면 이는 $$I$$ 만큼의 정보를 표현하기 위해서는 $$log_2 I$$ 만큼의 비트를 필요로 하다는 뜻이기도 합니다. 

위에서 정보(I)를 '정보 엔트로피'(Bit)라는 정보량의 단위로 표현한다고 했으니, 섀넌의 이론에 따르면 $$log_2 I$$를 $$I$$에 대한 정보량이라고 부를 수 있을 것입니다.

## 확률론적으로 본 사건의 정보량

이미 일어난 사건에 대한 정보량은 위처럼 표현할 수 있겠지만, 정보란 발생하지 않은 사건을 표현할 때 더 가치 있습니다. 하지만 발생하지 않은 사건은 과거의 경험을 통해 얻은 '발생할 확률'만을 가지고 있습니다.

그렇다면 발생 확률이 $$x$$인 사건의 정보량을 수학적으로 어떻게 표현할 수 있을까요?  
섀넌은 다음과 같은 수식을 제안합니다.

$$I(x) = -\log_2 P(x)$$

위에서 구한 사건의 정보량 수식에 확률 개념을 더하고, **음수**를 취해서 '자주 발생하지 않는 사건'일수록 더 큰 값을 갖도록 하였습니다. 간단하지만 논리적으로 정보량을 표현하고 있습니다.

<p align="center" style="color:gray"><img src="/assets/images/Two_red_dice_01.svg">By Stephen Silver</p>


주사위를 던져 특정 눈이 나오는 경우를 통해 정보량을 표현하는 예시를 들어보겠습니다.  
사건 A는 주사위를 던져 1의 눈이 나오는 경우입니다. $$P(A)=\frac{1}{6}$$
사건 B는 주사위를 던져 짝수의 눈이 나오는 경우입니다. $$P(B)=\frac{3}{6}$$

이 두 사건을 위 섀넌의 정보량 식에 대입하면 각각 $$I(A)=2.5849$$, $$I(B)=1$$이 됩니다. 즉, 발생할 확률이 적은 사건 A가 더 높은 정보량을 가지게 되는 것입니다.

사건 A와 B의 정보량을 정량화한 $$2.5849$$ 비트 와 $$1$$ 비트는 **답을 구하기 위한 질문의 개수의 기댓값**으로도 해석할 수 있습니다. 예를 들어 사건 A에서 주사위를 던졌을 때 나온 눈을 맞추기 위해서는  
Q1. 주사위의 눈이 3 이하입니까? A1. 네  
Q2. 주사위의 눈이 2 이상입니까? A2. 아니요.
=> 주사위의 눈은 1 입니다.  
와 같은 질문이 필요합니다. Q2 에서 '이상' 혹은 '이하'를 선택하는 것에 따라 필요한 질문은 2 개 혹은 3 개가 됩니다.  
사건 B에서는 Q1. 주사위의 눈은 홀수입니까? 라는 하나의 질문으로 답을 구할 수 있습니다.

이처럼 정보량은 답을 찾기 위해 몇번의 질문이 필요한 지를 수치화한, 문제의 불확실함의 정도라고 할 수 있습니다.

## 확률 분포에서 본 정보량

내일 날씨를 예상할 때 우리는 주위 정보를 활용합니다. 예를 들어 날씨가 흐리니 내일은 비가 오겠지 하는 것입니다. 이때 내일 날씨에 대한 확률은 [비: 50%, 흐림: 30%, 맑음: 20%] 라고 표현할 수 있습니다. 하지만 만약 건물 지하에서 온종일 갇혀 지내서 아무 정보를 얻을 수 없었다면  내일 날씨에 대해 모두 동일한 확률값 [비: 33%, 흐림: 33%, 맑음: 33%]을 부여할 것입니다.

내일 날씨와 같은 확률 분포의 정보량은 다음과 같이 계산할 수 있습니다.  

$$H = \sum{(사건~발생~확률})*(사건의~정보량) $$  
$$ =\sum_{i}{p_i}*I(x_i) $$  
$$ =-\sum_{i}p_i \log_2{p_i}$$

위 식을 사용하여이 내일 날씨 확률 분포에 대한 정보량을 계산하면, 전자의 경우 $$0.5 + 0.521 + 0.4643 = 1.4853$$이 되고, 후자의 경우 $$0.5283 + 0.5283 + 0.5283 = 1.5849$$가 됩니다.

<p align="center"><img src="/assets/images/distribution_entropy.png"></p>
<p align="center" style="color:gray">Pattern Recognition and Machine Learning, C.M. Bishop</p>


위와 같은 특징 때문에 엔트로피는 사건의 발생 확률이 모든 경우에 대해 동일할 때 가장 큰 값을 갖습니다. 사건의 발생 확률이 모든 경우에 동일하다는 것은 곧 그 사건에 대한 아무런 정보가 없다는 뜻이고, 이는 정보의 불확실성, 즉 정보량이 크다는 것을 의미합니다.

## 정보 압축

알파벳 26 자와 공백까지 포함한 총 27 가지의 문자가 등장하는 경우를 생각해봅시다. 만약 모든 문자가 동일한 확률로 발생한다고 가정했을 때, 한 글자의 정보량은 $$\log_{2}27 = 4.7548$$ 입니다. 이 정보량을 낮추기 위해 무엇을 할 수 있을까요?

바로 과거에 발생하여 우리가 알고 있는 정보, 즉 **확률**을 도입하여 이 정보량을 줄일 수 있습니다. (흥미롭게도 Maximum A Posterior의 prior과도 일부 닿아있는 모습을 확인할 수 있네요.) 통계적으로 알파벳은 e의 등장 확률이 가장 높고(0.1031), z의 등장 확률이 가장 낮습니다(0.0005). 이와 같이 각 알파벳의 등장 확률을 기반으로 한 글자의 정보량을 계산해보면 약 **4.08 비트**가 됩니다.

이처럼 각 상태의 확률 분포(=추가 정보)를 도입한다면 정보량을 줄일 수 있습니다. 이렇게 구한 **실제 정보량**(4.08)을 **최대 정보량**(4.7548)으로 나눈 값(0.848)을 **상대 정보량 relative entropy** 이라고 하고, 1에서 상대 정보량을 뺀 값(0.152)을 **리던던시 Redundancy** 라고 합니다.

리던던시는 여분, 과잉의 뜻을 가지는 단어로, 정보 이론에서는 "그것이 없어도 전체 정보의 본질적인 뜻이 변하지 않는 정보"를 의미합니다. 따라서 모든 상태가 무작위인 최대 정보량에서부터 최대한 큰 리던던시를 제거하여 실제 정보량을 줄이는 것이 정보 압축의 핵심 개념이라 할 수 있습니다.

상태의 발생 확률을 조정함으로써 정보를 압축하는 방법을 1차 근사라고 합니다. 이외에도 정보량을 줄이는 다양한 방법들이 있습니다. 그 중 하나는 이전 상태의 정보를 참조하는 것으로, 예를 들어 영단어에서 이전 글자가 q 인 경우, 그 다음 글자는 u 혹은 a가 올 확률이 매우 높을 것입니다. 

## 마무리

섀넌은 통신의 효율화를 위해 리던던시를 제거해 실제 정보량을 낮추는 방안을 제시하였습니다. 그는 정보량 계산식을 통해 데이터 압축이 가능함과 그 한계가 존재한다는 점, 그리고 그 한계를 넘어선 압축이 일어날 경우 원래의 정보를 완벽하게 복원하는 것은 불가능하다는 것을 증명하였습니다.

머신러닝에서는 cross-entropy, KL-Divergence 등의 손실 함수를 정의하는 데에 이 개념이 사용됩니다. 

## 참고자료

<details><summary>펼치기</summary>
<div markdown="1">
- https://ratsgo.github.io/statistics/2017/09/22/information/
- https://horizon.kias.re.kr/18474/
- https://hyunw.kim/blog/2017/10/14/Entropy.html
- http://www.aistudy.co.kr/control/information_theory.htm

  
</div>
</details>
